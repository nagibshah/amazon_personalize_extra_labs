{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77828a04",
   "metadata": {},
   "source": [
    "# [Amazon Personalize](https://aws.amazon.com/personalize/) introduces - Trending Now\n",
    "\n",
    "This notebook will walk you through an example of using Trending Now recipe in [Amazon Personalize](https://aws.amazon.com/personalize/)\n",
    "\n",
    "User interests can change based on a variety of factors, such as external events or the interests of other users. It is critical for websites and apps to tailor their recommendations to these changing interests to improve user engagement. With [Trending-Now](https://docs.aws.amazon.com/personalize/latest/dg/native-recipe-trending-now.html), you can surface items from your catalogue that are rising in popularity faster with higher velocity than other items, such as trending news, popular social content or newly released movies. Amazon Personalize looks for items that are rising in popularity at a faster rate than other catalogue items to help users discover items that are engaging their peers. Amazon Personalize also allows customers to define the time periods over which trends are calculated depending on their unique business context, with options for every 30 mins, 1 hour, 3 hours or 1 day, based on the most recent interactions data from users.\n",
    "This notebook will demonstrate how the new recipe aws-trending-now (or aws-vod-trending-now for recommenders) can help recommend the top trending items from the interactions dataset.  \n",
    "\n",
    "The estimated time to run through this notebook is about 60 minutes.  \n",
    "\n",
    "## How to use the Notebook\n",
    "\n",
    "The code is broken up into cells like the one below. There's a triangular Run button at the top of this page that you can click to execute each cell and move onto the next, or you can press `Shift` + `Enter` while in the cell to execute it and move onto the next one.\n",
    "\n",
    "As a cell is executing you'll notice a line to the side showcase an `*` while the cell is running or it will update to a number to indicate the last cell that completed executing after it has finished exectuting all the code within a cell.\n",
    "\n",
    "Simply follow the instructions below and execute the cells to get started.\n",
    "\n",
    "## Introduction to Amazon Personalize\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/personalize/) is a fully managed machine learning service that makes it easy for developers to deliver personalized experiences to their users. It enables you to improve customer engagement by powering personalized product and content recommendations in websites, applications, and targeted marketing campaigns. You can get started without any prior machine learning (ML) experience, using APIs to easily build sophisticated personalization capabilities in a few clicks. All your data is encrypted to be private and secure, and is only used to create recommendations for your users. \n",
    "\n",
    "You can start using Amazon Personalize with a simple three step process, which only takes a few clicks in the AWS console, or a set of simple API calls. \n",
    "\n",
    "First, point Amazon Personalize to user data, catalog data, and activity stream of views, clicks, purchases, etc. in Amazon S3 or upload using a simple API call. \n",
    "\n",
    "Second, with a single click in the console or an API call, train a private recommendation model for your data. \n",
    "\n",
    "Third, retrieve personalized recommendations for any user by creating a recommender, and using the GetRecommendations API.\n",
    "\n",
    "If you are not familiar with Amazon Personalize, you can learn more about the service on by looking at [Github Sample Notebooks](https://github.com/aws-samples/amazon-personalize-samples) and [Product Documentation](https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ca987",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Python ships with a broad collection of libraries and we need to import those as well as the ones installed to help us like [boto3](https://aws.amazon.com/sdk-for-python/) (AWS SDK for python) and [Pandas](https://pandas.pydata.org/)/[Numpy](https://numpy.org/) which are core data science tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest version of botocore to ensure we have the latest features in the SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import boto3\n",
    "import json as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765d343",
   "metadata": {},
   "source": [
    "## Specify an S3 Bucket and Data Output Location\n",
    "\n",
    "Amazon Personalize will need an S3 bucket to act as the source of your data. The code bellow will create a bucket with a unique `bucket_name`.\n",
    "\n",
    "The Amazon S3 bucket needs to be in the same region as the Amazon Personalize resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the same region as current Amazon SageMaker Notebook\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print('region:', region)\n",
    "\n",
    "# Or you can specify the region where your bucket and model will be domiciled\n",
    "# region = \"us-east-1\" \n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = account_id + \"-\" + region + \"-\" + \"personalize-trending-now\"\n",
    "print('bucket_name:', bucket_name)\n",
    "\n",
    "try: \n",
    "    if region == \"us-east-1\":\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket = bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print(\"Bucket already exists. Using bucket\", bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Personalize:\n",
    "personalize = boto3.Session(region_name=region).client('personalize')\n",
    "personalize_events = boto3.Session(region_name=region).client('personalize-events')\n",
    "personalize_runtime = boto3.Session(region_name=region).client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a7e10",
   "metadata": {},
   "source": [
    "## Download, Prepare, and Upload Training Data\n",
    "\n",
    "For this notebook walkthrough, we will use Movielens public dataset, available at https://grouplens.org/datasets/movielens/. Follow the link to learn more about the data and potential uses.\n",
    "\n",
    "First we need to download the data (training data). In this tutorial, for the interactions data we will be using ratings history from the movies review dataset, MovieLens. The dataset contains the user_id, rating, item_id, the interactions between the users and items and the time this interaction took place (timestamp which is given as unix epoch time). The dataset also contains movie title information to map the movie id to the actual title and genres. \n",
    "\n",
    "### Download and Explore the Interactions and Items Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"blog_data\"\n",
    "!mkdir $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $data_dir && wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!cd $data_dir && unzip ml-latest-small.zip\n",
    "dataset_dir = data_dir + \"/ml-latest-small/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48f5c9",
   "metadata": {},
   "source": [
    "The dataset has been successfully downloaded \n",
    "\n",
    "Lets learn more about the dataset by viewing its charateristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c032ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize $dataset_dir/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bff4c",
   "metadata": {},
   "source": [
    "From the README, we see there is a file ratings.csv that should work as a proxy for our interactions data, after all rating a film definitely is a form of interacting with it. The dataset also has some genre information as some movie genome data. In this POC we will focus on the interactions data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(dataset_dir + '/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e5c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4064f7",
   "metadata": {},
   "source": [
    "Now lets create titles dataframe with the movie titles information which we will use for post processing the recommendations output for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a161598",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = pd.read_csv(dataset_dir + '/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129a14e",
   "metadata": {},
   "source": [
    "Renaming the columns to perform the JOIN operations in the later steps of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf989ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = titles_df.rename(columns = {'movieId':'ITEM_ID', 'title':'TITLE', 'genres':'GENRES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2706f7",
   "metadata": {},
   "source": [
    "## Prepare the Interactions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabb0c6",
   "metadata": {},
   "source": [
    "### Convert the ratings data to Click and Watch event interactions\n",
    "\n",
    "Since this is a dataset of an explicit feedback movie ratings, it includes movies rated from 1 to 5. We want to include only moves that were \"liked\" by the users, and simulate a dataset of data that would be gathered by a VOD platform. In order to do that, we will filter out all interactions under 2 out of 5, and create two event types: \"Click\" and and \"Watch\". We will then assign all movies rated 2 and above as \"Click\" and movies rated 4 and above as both \"Click\" and \"Watch\".\n",
    "\n",
    "Note that for a real data set you would actually model based on implicit feedback such as clicks, watches and/or explicit feedback such as ratings, likes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "watched_df = ratings_df.copy()\n",
    "watched_df = watched_df[watched_df['rating'] > 3]\n",
    "watched_df = watched_df[['userId', 'movieId', 'timestamp']]\n",
    "watched_df['EVENT_TYPE']='Watch'\n",
    "watched_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7138cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_df = ratings_df.copy()\n",
    "clicked_df = clicked_df[clicked_df['rating'] > 1]\n",
    "clicked_df = clicked_df[['userId', 'movieId', 'timestamp']]\n",
    "clicked_df['EVENT_TYPE']='Click'\n",
    "clicked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = clicked_df.copy()\n",
    "interactions_df = interactions_df.append(watched_df)\n",
    "interactions_df.sort_values(\"timestamp\", axis = 0, ascending = True, \n",
    "                 inplace = True, na_position ='last') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef18c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383fa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = interactions_df.rename(columns = {'userId':'USER_ID', 'movieId':'ITEM_ID', 'timestamp':'TIMESTAMP'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_file_path = 'curated_interactions_training_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13d9db",
   "metadata": {},
   "source": [
    "In the cell below, we will write our cleaned data to a file named \"final_training_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.to_csv(interactions_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966bbd6",
   "metadata": {},
   "source": [
    "## Configure an S3 bucket and an IAM role\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data onto the Amazon EBS instance attached to instance running this Jupyter notebook. However, Amazon Personalize will need an S3 bucket to act as the source of your data, as well as IAM roles for accessing that bucket. Let's set all of that up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b13fe",
   "metadata": {},
   "source": [
    "## Set the S3 bucket policy\n",
    "Amazon Personalize needs to be able to read the contents of your S3 bucket. So add a bucket policy which allows that.\n",
    "\n",
    "Note: Make sure the role you are using to run the code in this notebook has the necessary permissions to modify the S3 bucket policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket_name),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket_name)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2884e83",
   "metadata": {},
   "source": [
    "### Upload Interactions data to S3\n",
    "Now that our training data is ready for Amazon Personalize,the next step is to upload it to the s3 bucket created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ee211",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(interactions_file_path).upload_file(interactions_file_path)\n",
    "interactions_s3DataPath = \"s3://\"+bucket_name+\"/\"+interactions_file_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe117d",
   "metadata": {},
   "source": [
    "## Create and Wait for Dataset Group\n",
    "The largest grouping in Personalize is a Dataset Group, this will isolate your data, event trackers, solutions, Recommenders, and campaigns. Grouping things together that share a common collection of data. Feel free to alter the name below if you'd like. \n",
    "\n",
    "When you create a Domain dataset group, you choose your domain. The domain you specify determines the default schemas for datasets and the use cases that are available for recommenders. \n",
    "\n",
    "You can find more information about creating a Domain dataset group in [the documentation](https://docs.aws.amazon.com/personalize/latest/dg/create-domain-dataset-group.html).\n",
    "\n",
    "### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeed98c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = personalize.create_dataset_group(\n",
    "    name='trending_now_dataset_group'\n",
    ")\n",
    "\n",
    "dataset_group_arn = response['datasetGroupArn']\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bcc10",
   "metadata": {},
   "source": [
    "Wait for Dataset Group to Have ACTIVE Status\n",
    "Before we can use the Dataset Group in any items below it must be active, execute the cell below and wait for it to show active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b6c37",
   "metadata": {},
   "source": [
    "## Create Interactions Schema\n",
    "A core component of how Personalize understands your data comes from the Schema that is defined below. This configuration tells the service how to digest the data provided via your CSV file. Note the columns and types align to what was in the file you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\", # \"Watch\", \"Click\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }  \n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"trending-now-interaction-dataset-schema\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "interaction_schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd41ad",
   "metadata": {},
   "source": [
    "## Create Datasets\n",
    "After the group, the next thing to create is the datasets where your data will be uploaded to in Amazon Personalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad88e7d",
   "metadata": {},
   "source": [
    "### Create Interactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    name = \"trending-now-interactions\",\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = interaction_schema_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c72c02",
   "metadata": {},
   "source": [
    "## Create Personalize Role\n",
    "Also Amazon Personalize needs the ability to assume Roles in AWS in order to have the permissions to execute certain tasks, the lines below grant that.\n",
    "\n",
    "Note: Make sure the role you are using to run the code in this notebook has the necessary permissions to create a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeRoleTrendingNow\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n",
    "# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n",
    "# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = policy_arn\n",
    ")\n",
    "\n",
    "# Now add S3 support\n",
    "iam.attach_role_policy(\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    RoleName=role_name\n",
    ")\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f86ce",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "Earlier you created the DatasetGroup and Dataset to house your information, now you will execute an import job that will load the data from S3 into Amazon Personalize for usage building your model.\n",
    "### Create Interactions Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e902d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interactions_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"trending_now_interactions_import\",\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket_name, interactions_file_path)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_interactions_import_job_arn = create_interactions_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_interactions_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06519930",
   "metadata": {},
   "source": [
    "### Wait for Dataset Import Jobs to Have ACTIVE Status\n",
    "It can take a while before the import jobs complete, please wait until you see that they are active below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26369f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_interactions_import_job_arn\n",
    "    )\n",
    "    status = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    print(\"InteractionsDatasetImportJob: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08217d",
   "metadata": {},
   "source": [
    "## Training the Trending-Now model - Solution & Solution Version\n",
    "With the interactions dataset imported into dataset group, we will next create solution and solution version using the trending-now recipe. First, let's initialize the trending-now-recipe for creating the custom solution. Please note the new featureTransformationParameter that we are setting for this recipe while creating the solution. Its value can be selected from the set of [30 minutes, 1 hour, 3 hours, 1 day].\n",
    "\n",
    "This input parameter should be decided based on the data distribution for each input interactions dataset. If the use case falls in video domain, where the trending movies or TV series change every 1 day, then we should select 1 day (or) if the use case falls in news domain, then we should select 30 minute so that fast changing news stories are showcased as trending items in recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_now_recipe = \"arn:aws:personalize:::recipe/aws-trending-now\"\n",
    "solution_response = personalize.create_solution(name=\"trending-now-solution\", \n",
    "                                                recipeArn=trending_now_recipe, \n",
    "                                                datasetGroupArn=dataset_group_arn,\n",
    "                                                solutionConfig = {\n",
    "                                                    \"featureTransformationParameters\": {\n",
    "                                                        'trend_discovery_frequency': '30 minutes'\n",
    "                                                    }\n",
    "                                                }   \n",
    "                                               )\n",
    "solution_arn = solution_response['solutionArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_version_response = personalize.create_solution_version(solutionArn=solution_arn, \n",
    "                                                         trainingMode=\"FULL\")\n",
    "solution_version_arn = solution_version_response[\"solutionVersionArn\"]\n",
    "print(json.dumps(solution_version_response, indent=4, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_solution_versions = [solution_version_arn]\n",
    "\n",
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "    for solution_version_arn in in_progress_solution_versions:\n",
    "        version_response = personalize.describe_solution_version(\n",
    "            solutionVersionArn = solution_version_arn\n",
    "        )\n",
    "        status = version_response[\"solutionVersion\"][\"status\"]\n",
    "        \n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(solution_version_arn))\n",
    "            in_progress_solution_versions.remove(solution_version_arn)\n",
    "    \n",
    "    if len(in_progress_solution_versions) <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Solution Version is still IN_PROGRESS state\")\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc774f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize.describe_solution_version(solutionVersionArn=solution_version_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598101e",
   "metadata": {},
   "source": [
    "## Campaign\n",
    "With solution and solution version created, we will now create a campaign to get recommendations. Please note, with trending now recipe, we can use both [filter](https://docs.aws.amazon.com/personalize/latest/dg/filter.html) and [promotions](https://docs.aws.amazon.com/personalize/latest/dg/promoting-items.html) while calling getRecommendations over an active campaign (or recommender)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_response = personalize.create_campaign(\n",
    "    name=\"trending-now-campaign\",\n",
    "    solutionVersionArn=solution_version_arn,\n",
    "    minProvisionedTPS=1,\n",
    ")\n",
    "campaign_arn = campaign_response['campaignArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_campaigns = [campaign_arn]\n",
    "\n",
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "while time.time() < max_time:\n",
    "    for campaign_arn in in_progress_campaigns:\n",
    "        describe_campaign_response = personalize.describe_campaign(\n",
    "            campaignArn = campaign_arn\n",
    "        )\n",
    "        status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "        \n",
    "        if status == \"ACTIVE\":\n",
    "            print(\"Build succeeded for {}\".format(campaign_arn))\n",
    "            in_progress_campaigns.remove(campaign_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(\"Build failed for {}\".format(campaign_arn))\n",
    "            in_progress_campaigns.remove(campaign_arn)\n",
    "    \n",
    "    if len(in_progress_campaigns) <= 0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Campaign is still IN_PROGRESS state\")\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0413b0",
   "metadata": {},
   "source": [
    "## Getting recommendations\n",
    "Now that the campaign has been trained, lets have a look at the recommendations we can get for our users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a78de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = personalize_runtime.get_recommendations(campaignArn=campaign_arn, \n",
    "                                                   numResults=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d47e30",
   "metadata": {},
   "source": [
    "The results from the GetRecommendations call includes the id’s of recommended items. Below is the code to map the id’s to the actual movie titles for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for item in item_ids[\"itemList\"]:\n",
    "    items.append(int(item[\"itemId\"]))\n",
    "recommended_items_df = pd.DataFrame (items, columns = ['ITEM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommended_items_df.merge(titles_df, on=\"ITEM_ID\")\n",
    "recommendations = recommendations[[\"ITEM_ID\", \"TITLE\"]]\n",
    "recommendations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f24e",
   "metadata": {},
   "source": [
    "## EventTracker to ingest Put Events to trigger auto-updates\n",
    "To use this recipe effectively and to catch trendiness for the 'Trend discovery frequency' configured - one should use putEvents API to continuously ingest new events into the system. Trending now model will incorporate these new events with automatic model updates (at no cost to end customer) to show latest trending now items.\n",
    "Lets create an event tracker to ingest new events into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d52d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_tracker_response = personalize.create_event_tracker(\n",
    "    name='trending-now-event-tracker',\n",
    "    datasetGroupArn=dataset_group_arn\n",
    ")\n",
    "tracking_id = event_tracker_response['trackingId']\n",
    "event_tracker_response\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bdddc8",
   "metadata": {},
   "source": [
    "### Prepare latest interactions  \n",
    "Let us create some dummy latest interaction events with some random user_ids and item_ids to push it to the Amaon Personalize using event tracker. We will use the interaction timestamp to be of the current timestamp to replicate latest real time ingestions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rand = interactions_df['USER_ID'].sample(10)\n",
    "items_rand = interactions_df['ITEM_ID'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7709d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(user_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7377d8",
   "metadata": {},
   "source": [
    "Above are the randomly selected USER_ID’s that are used for generating incremental interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(items_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed28a64b",
   "metadata": {},
   "source": [
    "Above are the randomly selected ITEM_ID’s that are used for generating incremental interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61647520",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[titles_df['ITEM_ID'].isin(list(items_rand))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,25):\n",
    "    for user_id in user_rand:\n",
    "        session_id = str(uuid.uuid4())\n",
    "        for item_id in items_rand:\n",
    "            time.sleep(1)\n",
    "            time_epoch = int(time.time())\n",
    "            response = personalize_events.put_events(\n",
    "                trackingId=tracking_id,\n",
    "                userId=str(user_id),\n",
    "                sessionId=session_id,\n",
    "                eventList=[\n",
    "                    {\n",
    "                        'eventType': 'Watch',\n",
    "                        'itemId': str(item_id),\n",
    "                        'sentAt': time_epoch\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            print (\"Iteration: %s, User: %s, Item: %s, EpochTime: %s\" %(i, user_id, item_id, str(time_epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576895ae",
   "metadata": {},
   "source": [
    "Approximately it takes 40+ minutes to push all the dummy interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620930d7",
   "metadata": {},
   "source": [
    "After you push the incremental interactions dataset, wait for Trend Discovery Frequency time configured plus additional processing time (15 minutes) for the new recommendations to get reflected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114960f2",
   "metadata": {},
   "source": [
    "## Getting trending-now recommendations\n",
    "Now that the latest interactions are pushed to Personalize, lets have a look at the latest trending now recommendations we can get for our users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = personalize_runtime.get_recommendations(campaignArn=campaign_arn, \n",
    "                                                   numResults=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for item in item_ids[\"itemList\"]:\n",
    "    items.append(int(item[\"itemId\"]))\n",
    "recommended_items_df = pd.DataFrame (items, columns = ['ITEM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = recommended_items_df.merge(titles_df, on=\"ITEM_ID\")\n",
    "recommendations = recommendations[[\"ITEM_ID\", \"TITLE\"]]\n",
    "recommendations.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e01eaa",
   "metadata": {},
   "source": [
    "## Review\n",
    "The above GetRecommendations call includes the id’s of recommended items. Now we see the ITEM_ID’s recommended are from the incremental interactions dataset that we had provided to the Personalize model. Amazon Personalize trending now recipe looks for items that are rising in popularity at a faster rate in the latest interactions that are streaming in and recommend them as trending now items.\n",
    "Using the notebook code above you have successfully trained a trending now model to generate item recommendations  that are rapidly becoming popular with your users and tailor your recommendations for the fast changing trends in the user interest. \n",
    "Going forward, you can adapt this code to create other recommenders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa077309",
   "metadata": {},
   "source": [
    "## Cleanup Resources \n",
    "This section contains instructions on how to clean up the resources created in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('dataset_group_arn:', dataset_group_arn)\n",
    "print ('role_name:', role_name)\n",
    "print ('region:', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb543a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import getopt\n",
    "import logging\n",
    "import botocore\n",
    "import boto3\n",
    "import time\n",
    "from packaging import version\n",
    "from time import sleep\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "personalize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset_group_arn(dataset_group_name):\n",
    "    dsg_arn = None\n",
    "\n",
    "    paginator = personalize.get_paginator('list_dataset_groups')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for dataset_group in paginate_result[\"datasetGroups\"]:\n",
    "            if dataset_group['name'] == dataset_group_name:\n",
    "                dsg_arn = dataset_group['datasetGroupArn']\n",
    "                break\n",
    "\n",
    "        if dsg_arn:\n",
    "            break\n",
    "\n",
    "    if not dsg_arn:\n",
    "        raise NameError(f'Dataset Group \"{dataset_group_name}\" does not exist; verify region is correct')\n",
    "\n",
    "    return dsg_arn\n",
    "\n",
    "def _get_solutions(dataset_group_arn):\n",
    "    solution_arns = []\n",
    "\n",
    "    paginator = personalize.get_paginator('list_solutions')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for solution in paginate_result['solutions']:\n",
    "            solution_arns.append(solution['solutionArn'])\n",
    "\n",
    "    return solution_arns\n",
    "\n",
    "def _delete_campaigns(solution_arns):\n",
    "    campaign_arns = []\n",
    "\n",
    "    for solution_arn in solution_arns:\n",
    "        paginator = personalize.get_paginator('list_campaigns')\n",
    "        for paginate_result in paginator.paginate(solutionArn = solution_arn):\n",
    "            for campaign in paginate_result['campaigns']:\n",
    "                if campaign['status'] in ['ACTIVE', 'CREATE FAILED']:\n",
    "                    logger.info('Deleting campaign: ' + campaign['campaignArn'])\n",
    "\n",
    "                    personalize.delete_campaign(campaignArn = campaign['campaignArn'])\n",
    "                elif campaign['status'].startswith('DELETE'):\n",
    "                    logger.warning('Campaign {} is already being deleted so will wait for delete to complete'.format(campaign['campaignArn']))\n",
    "                else:\n",
    "                    raise Exception('Campaign {} has a status of {} so cannot be deleted'.format(campaign['campaignArn'], campaign['status']))\n",
    "\n",
    "                campaign_arns.append(campaign['campaignArn'])\n",
    "\n",
    "    max_time = time.time() + 30*60 # 30 mins\n",
    "    while time.time() < max_time:\n",
    "        for campaign_arn in campaign_arns:\n",
    "            try:\n",
    "                describe_response = personalize.describe_campaign(campaignArn = campaign_arn)\n",
    "                logger.debug('Campaign {} status is {}'.format(campaign_arn, describe_response['campaign']['status']))\n",
    "            except ClientError as e:\n",
    "                error_code = e.response['Error']['Code']\n",
    "                if error_code == 'ResourceNotFoundException':\n",
    "                    campaign_arns.remove(campaign_arn)\n",
    "\n",
    "        if len(campaign_arns) == 0:\n",
    "            logger.info('All campaigns have been deleted or none exist for dataset group')\n",
    "            break\n",
    "        else:\n",
    "            logger.info('Waiting for {} campaign(s) to be deleted'.format(len(campaign_arns)))\n",
    "            time.sleep(20)\n",
    "\n",
    "    if len(campaign_arns) > 0:\n",
    "        raise Exception('Timed out waiting for all campaigns to be deleted')\n",
    "\n",
    "def _delete_solutions(solution_arns):\n",
    "    for solution_arn in solution_arns:\n",
    "        try:\n",
    "            describe_response = personalize.describe_solution(solutionArn = solution_arn)\n",
    "            solution = describe_response['solution']\n",
    "            if solution['status'] in ['ACTIVE', 'CREATE FAILED']:\n",
    "                logger.info('Deleting solution: ' + solution_arn)\n",
    "\n",
    "                personalize.delete_solution(solutionArn = solution_arn)\n",
    "            elif solution['status'].startswith('DELETE'):\n",
    "                logger.warning('Solution {} is already being deleted so will wait for delete to complete'.format(solution_arn))\n",
    "            else:\n",
    "                raise Exception('Solution {} has a status of {} so cannot be deleted'.format(solution_arn, solution['status']))\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code != 'ResourceNotFoundException':\n",
    "                raise e\n",
    "\n",
    "    max_time = time.time() + 30*60 # 30 mins\n",
    "    while time.time() < max_time:\n",
    "        for solution_arn in solution_arns:\n",
    "            try:\n",
    "                describe_response = personalize.describe_solution(solutionArn = solution_arn)\n",
    "                logger.debug('Solution {} status is {}'.format(solution_arn, describe_response['solution']['status']))\n",
    "            except ClientError as e:\n",
    "                error_code = e.response['Error']['Code']\n",
    "                if error_code == 'ResourceNotFoundException':\n",
    "                    solution_arns.remove(solution_arn)\n",
    "\n",
    "        if len(solution_arns) == 0:\n",
    "            logger.info('All solutions have been deleted or none exist for dataset group')\n",
    "            break\n",
    "        else:\n",
    "            logger.info('Waiting for {} solution(s) to be deleted'.format(len(solution_arns)))\n",
    "            time.sleep(20)\n",
    "\n",
    "    if len(solution_arns) > 0:\n",
    "        raise Exception('Timed out waiting for all solutions to be deleted')\n",
    "\n",
    "def _delete_event_trackers(dataset_group_arn):\n",
    "    event_tracker_arns = []\n",
    "\n",
    "    event_trackers_paginator = personalize.get_paginator('list_event_trackers')\n",
    "    for event_tracker_page in event_trackers_paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for event_tracker in event_tracker_page['eventTrackers']:\n",
    "            if event_tracker['status'] in [ 'ACTIVE', 'CREATE FAILED' ]:\n",
    "                logger.info('Deleting event tracker {}'.format(event_tracker['eventTrackerArn']))\n",
    "                personalize.delete_event_tracker(eventTrackerArn = event_tracker['eventTrackerArn'])\n",
    "            elif event_tracker['status'].startswith('DELETE'):\n",
    "                logger.warning('Event tracker {} is already being deleted so will wait for delete to complete'.format(event_tracker['eventTrackerArn']))\n",
    "            else:\n",
    "                raise Exception('Solution {} has a status of {} so cannot be deleted'.format(event_tracker['eventTrackerArn'], event_tracker['status']))\n",
    "\n",
    "            event_tracker_arns.append(event_tracker['eventTrackerArn'])\n",
    "\n",
    "    max_time = time.time() + 30*60 # 30 mins\n",
    "    while time.time() < max_time:\n",
    "        for event_tracker_arn in event_tracker_arns:\n",
    "            try:\n",
    "                describe_response = personalize.describe_event_tracker(eventTrackerArn = event_tracker_arn)\n",
    "                logger.debug('Event tracker {} status is {}'.format(event_tracker_arn, describe_response['eventTracker']['status']))\n",
    "            except ClientError as e:\n",
    "                error_code = e.response['Error']['Code']\n",
    "                if error_code == 'ResourceNotFoundException':\n",
    "                    event_tracker_arns.remove(event_tracker_arn)\n",
    "\n",
    "        if len(event_tracker_arns) == 0:\n",
    "            logger.info('All event trackers have been deleted or none exist for dataset group')\n",
    "            break\n",
    "        else:\n",
    "            logger.info('Waiting for {} event tracker(s) to be deleted'.format(len(event_tracker_arns)))\n",
    "            time.sleep(20)\n",
    "\n",
    "    if len(event_tracker_arns) > 0:\n",
    "        raise Exception('Timed out waiting for all event trackers to be deleted')\n",
    "\n",
    "def _delete_filters(dataset_group_arn):\n",
    "    filter_arns = []\n",
    "\n",
    "    filters_response = personalize.list_filters(datasetGroupArn = dataset_group_arn, maxResults = 100)\n",
    "    for filter in filters_response['Filters']:\n",
    "        logger.info('Deleting filter ' + filter['filterArn'])\n",
    "        personalize.delete_filter(filterArn = filter['filterArn'])\n",
    "        filter_arns.append(filter['filterArn'])\n",
    "\n",
    "    max_time = time.time() + 30*60 # 30 mins\n",
    "    while time.time() < max_time:\n",
    "        for filter_arn in filter_arns:\n",
    "            try:\n",
    "                describe_response = personalize.describe_filter(filterArn = filter_arn)\n",
    "                logger.debug('Filter {} status is {}'.format(filter_arn, describe_response['filter']['status']))\n",
    "            except ClientError as e:\n",
    "                error_code = e.response['Error']['Code']\n",
    "                if error_code == 'ResourceNotFoundException':\n",
    "                    filter_arns.remove(filter_arn)\n",
    "\n",
    "        if len(filter_arns) == 0:\n",
    "            logger.info('All filters have been deleted or none exist for dataset group')\n",
    "            break\n",
    "        else:\n",
    "            logger.info('Waiting for {} filter(s) to be deleted'.format(len(filter_arns)))\n",
    "            time.sleep(20)\n",
    "\n",
    "    if len(filter_arns) > 0:\n",
    "        raise Exception('Timed out waiting for all filter to be deleted')\n",
    "\n",
    "def _delete_datasets_and_schemas(dataset_group_arn):\n",
    "    dataset_arns = []\n",
    "    schema_arns = []\n",
    "\n",
    "    dataset_paginator = personalize.get_paginator('list_datasets')\n",
    "    for dataset_page in dataset_paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in dataset_page['datasets']:\n",
    "            describe_response = personalize.describe_dataset(datasetArn = dataset['datasetArn'])\n",
    "            schema_arns.append(describe_response['dataset']['schemaArn'])\n",
    "\n",
    "            if dataset['status'] in ['ACTIVE', 'CREATE FAILED']:\n",
    "                logger.info('Deleting dataset ' + dataset['datasetArn'])\n",
    "                personalize.delete_dataset(datasetArn = dataset['datasetArn'])\n",
    "            elif dataset['status'].startswith('DELETE'):\n",
    "                logger.warning('Dataset {} is already being deleted so will wait for delete to complete'.format(dataset['datasetArn']))\n",
    "            else:\n",
    "                raise Exception('Dataset {} has a status of {} so cannot be deleted'.format(dataset['datasetArn'], dataset['status']))\n",
    "\n",
    "            dataset_arns.append(dataset['datasetArn'])\n",
    "\n",
    "    max_time = time.time() + 30*60 # 30 mins\n",
    "    while time.time() < max_time:\n",
    "        for dataset_arn in dataset_arns:\n",
    "            try:\n",
    "                describe_response = personalize.describe_dataset(datasetArn = dataset_arn)\n",
    "                logger.debug('Dataset {} status is {}'.format(dataset_arn, describe_response['dataset']['status']))\n",
    "            except ClientError as e:\n",
    "                error_code = e.response['Error']['Code']\n",
    "                if error_code == 'ResourceNotFoundException':\n",
    "                    dataset_arns.remove(dataset_arn)\n",
    "\n",
    "        if len(dataset_arns) == 0:\n",
    "            logger.info('All datasets have been deleted or none exist for dataset group')\n",
    "            break\n",
    "        else:\n",
    "            logger.info('Waiting for {} dataset(s) to be deleted'.format(len(dataset_arns)))\n",
    "            time.sleep(20)\n",
    "\n",
    "    if len(dataset_arns) > 0:\n",
    "        raise Exception('Timed out waiting for all datasets to be deleted')\n",
    "\n",
    "    for schema_arn in schema_arns:\n",
    "        try:\n",
    "            logger.info('Deleting schema ' + schema_arn)\n",
    "            personalize.delete_schema(schemaArn = schema_arn)\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == 'ResourceInUseException':\n",
    "                logger.info('Schema {} is still in-use by another dataset (likely in another dataset group)'.format(schema_arn))\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    logger.info('All schemas used exclusively by datasets have been deleted or none exist for dataset group')\n",
    "\n",
    "def _delete_dataset_group(dataset_group_arn):\n",
    "    logger.info('Deleting dataset group ' + dataset_group_arn)\n",
    "    personalize.delete_dataset_group(datasetGroupArn = dataset_group_arn)\n",
    "\n",
    "    max_time = time.time() + 30*60 # 30 mins\n",
    "    while time.time() < max_time:\n",
    "        try:\n",
    "            describe_response = personalize.describe_dataset_group(datasetGroupArn = dataset_group_arn)\n",
    "            logger.debug('Dataset group {} status is {}'.format(dataset_group_arn, describe_response['datasetGroup']['status']))\n",
    "            break\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == 'ResourceNotFoundException':\n",
    "                logger.info('Dataset group {} has been fully deleted'.format(dataset_group_arn))\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        logger.info('Waiting for dataset group to be deleted')\n",
    "        time.sleep(20)\n",
    "\n",
    "def delete_dataset_groups(dataset_group_arns, region = None):\n",
    "    global personalize\n",
    "    personalize = boto3.client(service_name = 'personalize', region_name = region)\n",
    "\n",
    "    for dataset_group_arn in dataset_group_arns:\n",
    "        logger.info('Dataset Group ARN: ' + dataset_group_arn)\n",
    "\n",
    "        solution_arns = _get_solutions(dataset_group_arn)\n",
    "\n",
    "        # 1. Delete campaigns\n",
    "        _delete_campaigns(solution_arns)\n",
    "\n",
    "        # 2. Delete solutions\n",
    "        _delete_solutions(solution_arns)\n",
    "\n",
    "        # 3. Delete event trackers\n",
    "        _delete_event_trackers(dataset_group_arn)\n",
    "\n",
    "        # 4. Delete filters\n",
    "        _delete_filters(dataset_group_arn)\n",
    "\n",
    "        # 5. Delete datasets and their schemas\n",
    "        _delete_datasets_and_schemas(dataset_group_arn)\n",
    "\n",
    "        # 6. Delete dataset group\n",
    "        _delete_dataset_group(dataset_group_arn)\n",
    "\n",
    "        logger.info(f'Dataset group {dataset_group_arn} fully deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62166cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_dataset_groups([dataset_group_arn], region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.list_attached_role_policies(\n",
    "    RoleName = role_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detaching the role policies\n",
    "iam.detach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n",
    ")\n",
    "iam.detach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = 'arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6366d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.delete_role(\n",
    "    RoleName = role_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea782a",
   "metadata": {},
   "source": [
    "## Deleting the S3 bucket\n",
    "To delete an S3 bucket, it first needs to be empty. The easiest way to delete an S3 bucket, is just to navigate to S3 in the AWS console, delete the objects in the bucket, and then delete the S3 bucket itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d56931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
